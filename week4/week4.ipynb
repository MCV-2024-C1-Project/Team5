{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1 - Introduction to Human and Computer Vision\n",
    "## Week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Get project's root directory\n",
    "BASE_PATH = os.path.join(re.search(r'.+(Team5)', os.getcwd())[0], 'week4')\n",
    "os.chdir(BASE_PATH)\n",
    "BASE_PATH\n",
    "\n",
    "DATA_DIRECTORY = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pickle file to see detailed info of the images augmentation\n",
    "with open(f'{DATA_DIRECTORY}/qsd1_w4/augmentations.pkl', 'rb') as f:\n",
    "    augmentations_info = pickle.load(f)\n",
    "\n",
    "# Read pickle file with correspondences\n",
    "with open(f'{DATA_DIRECTORY}/qsd1_w4/gt_corresps.pkl', 'rb') as f:\n",
    "    ground_truth = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Remove background, detect noise (and filter it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:25<00:00,  2.85s/it]\n",
      "100%|██████████| 287/287 [00:03<00:00, 79.77it/s] \n"
     ]
    }
   ],
   "source": [
    "from src.background_removal import background_removal\n",
    "from src.noise_removal import denoise_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Image names\n",
    "QSD1_w4_names = [f for f in os.listdir(f'{DATA_DIRECTORY}/qsd1_w4/') if f.endswith('.jpg')]\n",
    "QSD1_w4_names.sort()\n",
    "\n",
    "# Initialize datasets\n",
    "BBDD = []\n",
    "QSD1_w4 = []\n",
    "QSD1_w4_filtered = []\n",
    "QSD1_w4_nonAugmented = []\n",
    "\n",
    "# Load datasets (+ filter)\n",
    "for image_name in tqdm(QSD1_w4_names):\n",
    "    # Read QSD1_w4\n",
    "    image_qsd1 = cv2.imread(f'{DATA_DIRECTORY}/qsd1_w4/{image_name}')\n",
    "    QSD1_w4.append(image_qsd1)\n",
    "\n",
    "    # Read non-augmented image\n",
    "    image_nonAugmented = cv2.imread(f'{DATA_DIRECTORY}/qsd1_w4/non_augmented/{image_name}')\n",
    "    image_nonAug_bckg_remov = background_removal(image_nonAugmented)  # Remove background in non-augmented image\n",
    "   \n",
    "    QSD1_w4_nonAugmented.append(image_nonAug_bckg_remov)\n",
    "\n",
    "    # Filter image from QSD1_w4\n",
    "    filtered_image = background_removal(denoise_image(image_qsd1))  # Detect noise (and clean it) + Remove background\n",
    "    QSD1_w4_filtered.append(filtered_image)\n",
    "\n",
    "\n",
    "# Read BBDD\n",
    "BBDD_names = [f for f in os.listdir(f'{DATA_DIRECTORY}/BBDD/') if f.endswith('.jpg')]\n",
    "BBDD_names.sort()\n",
    "\n",
    "for image_name in tqdm(BBDD_names):\n",
    "    image_bbdd = cv2.imread(f'{DATA_DIRECTORY}/BBDD/{image_name}')\n",
    "    BBDD.append(image_bbdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some examples\n",
    "img_number = 17\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3)  # 1 fila, 3 columnas\n",
    "\n",
    "# QSD1_w4 image\n",
    "image = QSD1_w4[img_number]\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('QSD1_w4 image')\n",
    "axes[0].axis('Off')\n",
    "\n",
    "# Filtered image (background removal + denoise)\n",
    "image = QSD1_w4_filtered[img_number][0]\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "axes[1].imshow(image)\n",
    "axes[1].set_title('Filtered image')\n",
    "axes[1].axis('Off')\n",
    "\n",
    "# Non-augmented image\n",
    "image = QSD1_w4_nonAugmented[img_number][0]\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "axes[2].imshow(image)\n",
    "axes[2].set_title('Non-augmented image')\n",
    "axes[2].axis('Off')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Detect keypoints and compute descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "flags = {\n",
    "  DEFAULT = 0,\n",
    "  DRAW_OVER_OUTIMG = 1,\n",
    "  NOT_DRAW_SINGLE_POINTS = 2,\n",
    "  DRAW_RICH_KEYPOINTS = 4\n",
    "}\n",
    "'''\n",
    "def draw_keypoints(image, kp, flags=0):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    img2 = cv2.drawKeypoints(gray_image, kp, None,(255,0,0),flags=flags)\n",
    "    plt.imshow(img2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sift_descriptor(image, params={}):\n",
    "    '''\n",
    "    Compute SIFT descriptors for a given image\n",
    "    :param image: image to compute the descriptors\n",
    "    :param params: parameters for the SIFT algorithm\n",
    "    :return: keypoints and descriptors\n",
    "\n",
    "    default params = {\n",
    "        'nfeatures': 0,\n",
    "        'nOctaveLayers': 3,\n",
    "        'contrastThreshold': 0.04,\n",
    "        'edgeThreshold': 10,\n",
    "        'sigma': 1.6\n",
    "    }\n",
    "    '''\n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create(**params)\n",
    "    kp, des = sift.detectAndCompute(img_gray, None)\n",
    "\n",
    "    return (kp, des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "    'nfeatures': 1500,\n",
    "    'nOctaveLayers': 3,\n",
    "}\n",
    "\n",
    "sift_query = []\n",
    "for picture in tqdm(QSD1_w4_filtered):\n",
    "    res = []\n",
    "    for painting in picture:\n",
    "        kp, des = sift_descriptor(painting, params=params)\n",
    "        res.append({'kp': kp, 'des': des})\n",
    "    sift_query.append(res)\n",
    "\n",
    "\n",
    "sift_bd = []\n",
    "for painting in tqdm(BBDD):\n",
    "    kp, des = sift_descriptor(painting, params=params)\n",
    "    sift_bd.append({'kp': kp, 'des': des})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = 16 # Query image\n",
    "idx2 = 11 # BBDD image\n",
    "\n",
    "des1 = sift_query[idx1][0]['des']\n",
    "des2 = sift_bd[idx2]['des']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "# Match descriptors.\n",
    "matches = bf.match(des1,des2)\n",
    "# Sort them in the order of their distance.\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "# Draw first 10 matches.\n",
    "img3 = cv2.drawMatches(\n",
    "    QSD1_w4_filtered[idx1][0], sift_query[idx1][0]['kp'],\n",
    "    BBDD[idx2], sift_bd[idx2]['kp'],\n",
    "    matches[:20], None,\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "print('Distance:', np.mean([m.distance for m in matches]))\n",
    "plt.imshow(img3), plt.axis('off'), plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "res = []\n",
    "dist_res = []\n",
    "for i in tqdm(range(30)):\n",
    "    picture_res = []\n",
    "    picture_distances = []\n",
    "    for j in range(0, len(sift_query[i])):\n",
    "        distances = []\n",
    "        for k in range(0, len(sift_bd)):\n",
    "            if sift_bd[k]['des'] is not None:\n",
    "                matches = bf.match(sift_query[i][j]['des'], sift_bd[k]['des'])\n",
    "                matches = sorted(matches, key = lambda x:x.distance)\n",
    "                # Take top 20 matches\n",
    "                distance = np.mean([match.distance for match in matches[:20]])\n",
    "                distances.append(distance)\n",
    "            else:\n",
    "                distances.append(10000)\n",
    "\n",
    "        # Take top 10 matches\n",
    "        most_similar = np.argsort(distances)[:10]\n",
    "        most_similar_distances = [distances[idx] for idx in most_similar]\n",
    "        if most_similar_distances[0] == 10000:\n",
    "            most_similar = [-1]\n",
    "            most_similar_distances = [10000]\n",
    "        if (\n",
    "            most_similar_distances[1] - most_similar_distances[0] < \n",
    "            2*np.mean(np.array(most_similar_distances[2:10]) - np.array(most_similar_distances[1:9]))\n",
    "        ):\n",
    "            most_similar = [-1]\n",
    "        picture_res.append(most_similar)\n",
    "        picture_distances.append(most_similar_distances)\n",
    "    \n",
    "    res.append(picture_res)\n",
    "    dist_res.append(picture_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = QSD1_w4_filtered[24][0]\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.metrics  # re-import the module to make sure it's recognized\n",
    "importlib.reload(src.metrics)\n",
    "\n",
    "from src.metrics import mapk\n",
    "\n",
    "# Now you can call the updated mapk function\n",
    "mapk(ground_truth, res, k=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "from scipy.spatial.distance import euclidean\n",
    "import importlib\n",
    "\n",
    "import src.metrics \n",
    "importlib.reload(src.metrics)\n",
    "from src.metrics import mapk\n",
    "\n",
    "params={\n",
    "    'shape': (128,128),  # Shape we want to resize the image to\n",
    "    'pixels_per_cell': (16,16),\n",
    "    'cells_per_block': (4,4),\n",
    "}\n",
    "\n",
    "def hog_descriptor(image, shape: tuple, pixels_per_cell: tuple, cells_per_block: tuple):\n",
    "    \n",
    "    # Image to grayscale\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize image (we need same dimensions in all the image descriptros to compare)\n",
    "    image = cv2.resize(image, shape)\n",
    "\n",
    "    # Compute HOG\n",
    "    hog_descriptor, hog_image = hog(image, pixels_per_cell=pixels_per_cell, cells_per_block=cells_per_block, visualize=True)\n",
    "    \n",
    "    # Rescale intensity, otherwise we may see everyhing black\n",
    "    hog_image = exposure.rescale_intensity(hog_image, in_range=(0, np.max(hog_image)/10))\n",
    "\n",
    "    return hog_descriptor, hog_image\n",
    "\n",
    "# Missing taking into account that there's an unknown class\n",
    "def compute_results_hog(hog_query, hog_bd, k=5):\n",
    "    res = []\n",
    "    distances_res = []\n",
    "\n",
    "    for i in tqdm(range(30)):\n",
    "        picture_res = []\n",
    "        picture_distances = []\n",
    "        \n",
    "        # For each painting inside an image from hog_query[i]\n",
    "        for j in range(len(hog_query[i])):\n",
    "            distances = []\n",
    "\n",
    "            query_descriptor = hog_query[i][j]['descriptor']\n",
    "            for p in range(len(hog_bd)): \n",
    "                bd_descriptor = hog_bd[p]['descriptor']\n",
    "                distance = euclidean(query_descriptor, bd_descriptor)\n",
    "                distances.append((distance, p))\n",
    "\n",
    "            distances.sort(key=lambda x: x[0])\n",
    "\n",
    "            nearest_neighbors = [idx for _, idx in distances[:k]] \n",
    "            nearest_distances = [round(dist, 2) for dist, _ in distances[:k]]\n",
    "\n",
    "            picture_res.append(nearest_neighbors)\n",
    "            picture_distances.append(nearest_distances)\n",
    "\n",
    "        res.append(picture_res)\n",
    "        distances_res.append(picture_distances)\n",
    "\n",
    "    return res, distances_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 120.86it/s]\n",
      "100%|██████████| 287/287 [00:01<00:00, 182.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Take HOG descriptor from Query and BBDD images\n",
    "\n",
    "hog_query = []\n",
    "for picture in tqdm(QSD1_w4_filtered):\n",
    "    res = []\n",
    "    for painting in picture:    \n",
    "        des, image = hog_descriptor(painting, **params)\n",
    "        res.append({'descriptor': des, 'image': image})\n",
    "    hog_query.append(res)\n",
    "\n",
    "hog_bd = []\n",
    "for painting in tqdm(BBDD):    \n",
    "    des, image = hog_descriptor(painting, **params)\n",
    "    hog_bd.append({'descriptor': des, 'image': image})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 301.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 [259, 283, 170, 36, 29]\n",
      "48 [48, 138, 64, 140, 286]\n",
      "251 [251, 11, 32, 259, 174]\n",
      "32 [32, 11, 94, 104, 40]\n",
      "161 [161, 40, 11, 104, 137]\n",
      "81 [81, 60, 11, 40, 241]\n",
      "62 [62, 64, 30, 25, 180]\n",
      "38 [38, 67, 102, 186, 118]\n",
      "-1 [259, 283, 36, 170, 264]\n",
      "128 [128, 241, 11, 105, 193]\n",
      "155 [155, 205, 60, 11, 40]\n",
      "258 [258, 29, 232, 101, 163]\n",
      "136 [136, 226, 271, 205, 140]\n",
      "76 [76, 142, 205, 249, 16]\n",
      "-1 [78, 12, 93, 107, 205]\n",
      "-1 [160, 45, 264, 259, 163]\n",
      "53 [53, 54, 254, 198, 226]\n",
      "-1 [197, 156, 146, 241, 35]\n",
      "12 [12, 93, 187, 221, 21]\n",
      "11 [259, 170, 36, 11, 282]\n",
      "280 [280, 51, 223, 57, 3]\n",
      "182 [51, 252, 182, 160, 101]\n",
      "252 [252, 38, 215, 91, 248]\n",
      "-1 [205, 16, 197, 78, 93]\n",
      "272 [272, 11, 32, 259, 163]\n",
      "117 [117, 60, 11, 104, 40]\n",
      "-1 [60, 31, 104, 40, 205]\n",
      "242 [242, 176, 93, 187, 157]\n",
      "260 [260, 29, 32, 162, 60]\n",
      "94 [94, 225, 261, 32, 192]\n",
      "132 [132, 11, 137, 163, 259]\n",
      "223 [223, 57, 101, 58, 215]\n",
      "-1 [93, 221, 12, 74, 156]\n",
      "127 [127, 11, 167, 215, 40]\n",
      "47 [47, 52, 279, 192, 225]\n",
      "13 [13, 90, 118, 38, 102]\n",
      "-1 [160, 163, 29, 45, 118]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute results\n",
    "res, distances_res = compute_results_hog(hog_query, hog_bd)\n",
    "\n",
    "mapk(ground_truth, res, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DAISY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import daisy\n",
    "\n",
    "def daisy_descriptor(image, step=250, radius=50, rings=3, histograms=6, orientations=8):\n",
    "    # Convertimos la imagen a escala de grises\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculamos los descriptores DAISY sin visualización\n",
    "    descriptors, dsc_image = daisy(\n",
    "        gray_image,\n",
    "        step=step,\n",
    "        radius=radius,\n",
    "        rings=rings,\n",
    "        histograms=histograms,\n",
    "        orientations=orientations,\n",
    "        visualize=True\n",
    "    )\n",
    "    \n",
    "    return descriptors, dsc_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daisy_descriptor(image, step=158, radius=50, rings=3, histograms=6, orientations=8):\n",
    "    # Convertimos la imagen a escala de grises\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Usamos un detector de keypoints (aquí FAST, pero puedes usar otros)\n",
    "    fast = cv2.FastFeatureDetector_create()\n",
    "    keypoints = fast.detect(gray_image, None)\n",
    "    \n",
    "    # Calculamos los descriptores DAISY en los puntos clave detectados\n",
    "    descriptors = daisy(\n",
    "        gray_image,\n",
    "        step=step,\n",
    "        radius=radius,\n",
    "        rings=rings,\n",
    "        histograms=histograms,\n",
    "        orientations=orientations,\n",
    "        visualize=False\n",
    "    )\n",
    "    \n",
    "    # Alineamos los descriptores con los keypoints (si es necesario)\n",
    "    keypoints_daisy = []\n",
    "    keypoint_descriptors = []\n",
    "    \n",
    "    for kp in keypoints:\n",
    "        # Para cada keypoint, calculamos su posición (en coordenadas de píxeles)\n",
    "        y, x = kp.pt\n",
    "        x, y = int(x), int(y)\n",
    "        \n",
    "        # Aquí usamos una verificación simple, que si el keypoint está dentro de la imagen\n",
    "        if x < descriptors.shape[1] and y < descriptors.shape[0]:\n",
    "            keypoints_daisy.append(kp)\n",
    "            keypoint_descriptors.append(descriptors[y, x, :])  # Tomamos el descriptor en esa ubicación\n",
    "    \n",
    "    # Convertimos las listas a arrays de numpy para facilitar el uso\n",
    "    keypoints_daisy = np.array(keypoints_daisy)\n",
    "    keypoint_descriptors = np.array(keypoint_descriptors)\n",
    "    \n",
    "    return keypoints_daisy, keypoint_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daisy_query = []\n",
    "for picture in tqdm(QSD1_w4_filtered[:4]):\n",
    "    for painting in picture:\n",
    "        kp, des = daisy_descriptor(painting)\n",
    "        daisy_query.append({'kp': kp, 'des': des})\n",
    "\n",
    "\n",
    "daisy_bd = []\n",
    "for picture in tqdm(QSD1_w4_nonAugmented[:4]):\n",
    "    for painting in picture:\n",
    "        kp, des = daisy_descriptor(painting)\n",
    "        daisy_bd.append({'kp': kp, 'des': des})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la imagen original y la visualización del descriptor DAISY\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('Imagen original')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(dsc_image, cmap='gray')\n",
    "plt.title('Descriptor DAISY')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 2\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image1, cmap='gray')\n",
    "plt.title('Imagen original')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(dsc_image1, cmap='gray')\n",
    "plt.title('Descriptor DAISY')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir descriptores a tipo float32 para el matcher de OpenCV\n",
    "descriptors = des_daisy.astype(np.float32).reshape(-1, 152)\n",
    "descriptors1 = des_daisy1.astype(np.float32).reshape(-1, 152)\n",
    "\n",
    "# Crear el matcher Brute-Force con la métrica de distancia Euclidiana\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "# Encontrar las mejores coincidencias\n",
    "matches = bf.match(descriptors, descriptors1)\n",
    "\n",
    "# Ordenar las coincidencias por distancia (las más cercanas son las mejores)\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "# Para dibujar los keypoints necesitamos crear un arreglo de KeyPoint\n",
    "# Generar keypoints para la primera y segunda imagen a partir de la malla DAISY\n",
    "keypoints = []\n",
    "keypoints1 = []\n",
    "\n",
    "step = 158  # El valor del 'step' usado en DAISY\n",
    "radius = 50  # El radio del descriptor\n",
    "\n",
    "# Generar keypoints para la imagen original\n",
    "for i in range(des_daisy.shape[0]):\n",
    "    for j in range(des_daisy.shape[1]):\n",
    "        keypoints.append(cv2.KeyPoint(j * step + radius, i * step + radius, 1))\n",
    "\n",
    "# Generar keypoints para la segunda imagen\n",
    "for i in range(des_daisy1.shape[0]):\n",
    "    for j in range(des_daisy1.shape[1]):\n",
    "        keypoints1.append(cv2.KeyPoint(j * step + radius, i * step + radius, 1))\n",
    "\n",
    "# Dibujar los matches entre las imágenes\n",
    "img_matches = cv2.drawMatches(\n",
    "    image, keypoints,  # Imagen original y sus keypoints\n",
    "    image1, keypoints1,  # Imagen de referencia y sus keypoints\n",
    "    matches[:30], None\n",
    ")\n",
    "\n",
    "# Mostrar el resultado con matplotlib\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img_matches)\n",
    "plt.title('Coincidencias entre imagen 1 y imagen 2 usando DAISY')\n",
    "plt.axis('off')  # Desactivar los ejes para que sea más limpio\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_query = []\n",
    "for picture in tqdm(QSD1_w4_filtered):\n",
    "    for painting in picture:\n",
    "        kp, des = surf_descriptor(painting)\n",
    "        surf_query.append({'kp': kp, 'des': des})\n",
    "\n",
    "\n",
    "surf_bd = []\n",
    "for picture in tqdm(QSD1_w4_nonAugmented):\n",
    "    for painting in picture:\n",
    "        kp, des = surf_descriptor(painting)\n",
    "        surf_bd.append({'kp': kp, 'des': des})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orb_descriptor(image, params={}):\n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    orb = cv2.ORB_create(**params)\n",
    "    kp, des = orb.detectAndCompute(img_gray, None)\n",
    "\n",
    "    return (kp, des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Find tentative matches based on similarity of local appearance and verify matches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Implement a system to discard queries not in the data set (unknowns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Evaluate system based on keypoint descriptors on QSD1-W4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Compare your best query system from previous week on QSD1-W4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Create pkl file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
