{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1 - Introduction to Human and Computer Vision\n",
    "## Week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Get project's root directory\n",
    "BASE_PATH = os.path.join(re.search(r'.+(Team5)', os.getcwd())[0], 'week4')\n",
    "os.chdir(BASE_PATH)\n",
    "BASE_PATH\n",
    "\n",
    "DATA_DIRECTORY = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pickle file to see detailed info of the images augmentation\n",
    "with open(f'{DATA_DIRECTORY}/qsd1_w4/augmentations.pkl', 'rb') as f:\n",
    "    augmentations_info = pickle.load(f)\n",
    "\n",
    "# Read pickle file with correspondences\n",
    "with open(f'{DATA_DIRECTORY}/qsd1_w4/gt_corresps.pkl', 'rb') as f:\n",
    "    ground_truth = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Remove background, detect noise (and filter it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:17<00:00,  2.58s/it]\n",
      "100%|██████████| 287/287 [00:03<00:00, 89.08it/s] \n"
     ]
    }
   ],
   "source": [
    "from src.background_removal import background_removal\n",
    "from src.noise_removal import denoise_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Image names\n",
    "QSD1_w4_names = [f for f in os.listdir(f'{DATA_DIRECTORY}/qsd1_w4/') if f.endswith('.jpg')]\n",
    "QSD1_w4_names.sort()\n",
    "\n",
    "# Initialize datasets\n",
    "BBDD = []\n",
    "QSD1_w4 = []\n",
    "QSD1_w4_filtered = []\n",
    "QSD1_w4_nonAugmented = []\n",
    "\n",
    "# Load datasets (+ filter)\n",
    "for image_name in tqdm(QSD1_w4_names):\n",
    "    # Read QSD1_w4\n",
    "    image_qsd1 = cv2.imread(f'{DATA_DIRECTORY}/qsd1_w4/{image_name}')\n",
    "    QSD1_w4.append(image_qsd1)\n",
    "\n",
    "    # Read non-augmented image\n",
    "    image_nonAugmented = cv2.imread(f'{DATA_DIRECTORY}/qsd1_w4/non_augmented/{image_name}')\n",
    "    image_nonAug_bckg_remov = background_removal(image_nonAugmented)  # Remove background in non-augmented image\n",
    "   \n",
    "    QSD1_w4_nonAugmented.append(image_nonAug_bckg_remov)\n",
    "\n",
    "    # Filter image from QSD1_w4\n",
    "    filtered_image = background_removal(denoise_image(image_qsd1))  # Detect noise (and clean it) + Remove background\n",
    "    QSD1_w4_filtered.append(filtered_image)\n",
    "\n",
    "\n",
    "# Read BBDD\n",
    "BBDD_names = [f for f in os.listdir(f'{DATA_DIRECTORY}/BBDD/') if f.endswith('.jpg')]\n",
    "BBDD_names.sort()\n",
    "\n",
    "for image_name in tqdm(BBDD_names):\n",
    "    image_bbdd = cv2.imread(f'{DATA_DIRECTORY}/BBDD/{image_name}')\n",
    "    BBDD.append(image_bbdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some examples\n",
    "img_number = 17\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3)  # 1 fila, 3 columnas\n",
    "\n",
    "# QSD1_w4 image\n",
    "image = QSD1_w4[img_number]\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('QSD1_w4 image')\n",
    "axes[0].axis('Off')\n",
    "\n",
    "# Filtered image (background removal + denoise)\n",
    "image = QSD1_w4_filtered[img_number][0]\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "axes[1].imshow(image)\n",
    "axes[1].set_title('Filtered image')\n",
    "axes[1].axis('Off')\n",
    "\n",
    "# Non-augmented image\n",
    "image = QSD1_w4_nonAugmented[img_number][0]\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "axes[2].imshow(image)\n",
    "axes[2].set_title('Non-augmented image')\n",
    "axes[2].axis('Off')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Detect keypoints and compute descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "flags = {\n",
    "  DEFAULT = 0,\n",
    "  DRAW_OVER_OUTIMG = 1,\n",
    "  NOT_DRAW_SINGLE_POINTS = 2,\n",
    "  DRAW_RICH_KEYPOINTS = 4\n",
    "}\n",
    "'''\n",
    "def draw_keypoints(image, kp, flags=0):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    img2 = cv2.drawKeypoints(gray_image, kp, None,(255,0,0),flags=flags)\n",
    "    plt.imshow(img2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sift_descriptor(image, params={}):\n",
    "    '''\n",
    "    Compute SIFT descriptors for a given image\n",
    "    :param image: image to compute the descriptors\n",
    "    :param params: parameters for the SIFT algorithm\n",
    "    :return: keypoints and descriptors\n",
    "\n",
    "    default params = {\n",
    "        'nfeatures': 0,\n",
    "        'nOctaveLayers': 3,\n",
    "        'contrastThreshold': 0.04,\n",
    "        'edgeThreshold': 10,\n",
    "        'sigma': 1.6\n",
    "    }\n",
    "    '''\n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create(**params)\n",
    "    kp, des = sift.detectAndCompute(img_gray, None)\n",
    "\n",
    "    return (kp, des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "    'nfeatures': 1500,\n",
    "    'nOctaveLayers': 3,\n",
    "}\n",
    "\n",
    "sift_query = []\n",
    "for picture in tqdm(QSD1_w4_filtered):\n",
    "    res = []\n",
    "    for painting in picture:\n",
    "        kp, des = sift_descriptor(painting, params=params)\n",
    "        res.append({'kp': kp, 'des': des})\n",
    "    sift_query.append(res)\n",
    "\n",
    "\n",
    "sift_bd = []\n",
    "for painting in tqdm(BBDD):\n",
    "    kp, des = sift_descriptor(painting, params=params)\n",
    "    sift_bd.append({'kp': kp, 'des': des})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = 16 # Query image\n",
    "idx2 = 11 # BBDD image\n",
    "\n",
    "des1 = sift_query[idx1][0]['des']\n",
    "des2 = sift_bd[idx2]['des']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "# Match descriptors.\n",
    "matches = bf.match(des1,des2)\n",
    "# Sort them in the order of their distance.\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "# Draw first 10 matches.\n",
    "img3 = cv2.drawMatches(\n",
    "    QSD1_w4_filtered[idx1][0], sift_query[idx1][0]['kp'],\n",
    "    BBDD[idx2], sift_bd[idx2]['kp'],\n",
    "    matches[:20], None,\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "print('Distance:', np.mean([m.distance for m in matches]))\n",
    "plt.imshow(img3), plt.axis('off'), plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "res = []\n",
    "dist_res = []\n",
    "for i in tqdm(range(30)):\n",
    "    picture_res = []\n",
    "    picture_distances = []\n",
    "    for j in range(0, len(sift_query[i])):\n",
    "        distances = []\n",
    "        for k in range(0, len(sift_bd)):\n",
    "            if sift_bd[k]['des'] is not None:\n",
    "                matches = bf.match(sift_query[i][j]['des'], sift_bd[k]['des'])\n",
    "                matches = sorted(matches, key = lambda x:x.distance)\n",
    "                # Take top 20 matches\n",
    "                distance = np.mean([match.distance for match in matches[:20]])\n",
    "                distances.append(distance)\n",
    "            else:\n",
    "                distances.append(10000)\n",
    "\n",
    "        # Take top 10 matches\n",
    "        most_similar = np.argsort(distances)[:10]\n",
    "        most_similar_distances = [distances[idx] for idx in most_similar]\n",
    "        if most_similar_distances[0] == 10000:\n",
    "            most_similar = [-1]\n",
    "            most_similar_distances = [10000]\n",
    "        if (\n",
    "            most_similar_distances[1] - most_similar_distances[0] < \n",
    "            2*np.mean(np.array(most_similar_distances[2:10]) - np.array(most_similar_distances[1:9]))\n",
    "        ):\n",
    "            most_similar = [-1]\n",
    "        picture_res.append(most_similar)\n",
    "        picture_distances.append(most_similar_distances)\n",
    "    \n",
    "    res.append(picture_res)\n",
    "    dist_res.append(picture_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = QSD1_w4_filtered[24][0]\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.metrics  # re-import the module to make sure it's recognized\n",
    "importlib.reload(src.metrics)\n",
    "\n",
    "from src.metrics import mapk\n",
    "\n",
    "# Now you can call the updated mapk function\n",
    "mapk(ground_truth, res, k=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "params={\n",
    "    'shape': (256,256),  # Shape we want to resize the image to\n",
    "    'pixels_per_cell': (8,8),\n",
    "    'cells_per_block': (2,2),\n",
    "}\n",
    "\n",
    "def hog_descriptor(image, shape: tuple, pixels_per_cell: tuple, cells_per_block: tuple):\n",
    "    \n",
    "    # Image to grayscale\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize image (we need same dimensions in all the image descriptros to compare)\n",
    "    image = cv2.resize(image, shape)\n",
    "\n",
    "    # Compute HOG\n",
    "    hog_descriptor, hog_image = hog(image, pixels_per_cell=pixels_per_cell, cells_per_block=cells_per_block, visualize=True)\n",
    "    \n",
    "    # Rescale intensity, otherwise we may see everyhing black\n",
    "    hog_image = exposure.rescale_intensity(hog_image, in_range=(0, np.max(hog_image)/10))\n",
    "\n",
    "    return hog_descriptor, hog_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 15.72it/s]\n",
      "100%|██████████| 287/287 [00:14<00:00, 19.82it/s]\n"
     ]
    }
   ],
   "source": [
    "hog_query = []\n",
    "original_query = [ ]  # Imatges del QSD1_W4 una vegada hem aplicat el background removal i en cas que hi hagi dos quadres els separem en dues imatges diferents\n",
    "for picture in tqdm(QSD1_w4_filtered):\n",
    "    res = []\n",
    "    for painting in picture: \n",
    "        original_query.append(painting)      \n",
    "        des, image = hog_descriptor(painting, **params)\n",
    "        res.append({'descriptor': des, 'image': image})\n",
    "    hog_query.append(res)\n",
    "\n",
    "hog_bd = []\n",
    "for painting in tqdm(BBDD):    \n",
    "    des, image = hog_descriptor(painting, **params)\n",
    "    hog_bd.append({'descriptor': des, 'image': image})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 41.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[40, 282, 11]], [], [[48, 254, 238], [251, 282, 11]], [[32, 11, 282]], [[161, 11, 40]], [[81, 40, 11]], [[54, 238, 202], [38, 182, 229]], [[282, 40, 32]], [[128, 54, 238]], [[155, 11, 282], [258, 215, 57]], [[136, 137, 54], [21, 92, 54]], [[78, 21, 74]], [[32, 11, 282]], [[53, 238, 54]], [[93, 197, 198]], [[12, 21, 74]], [[282, 11, 245], [232, 282, 32]], [], [[233, 282, 51], [215, 57, 259]], [[93, 198, 78]], [[272, 282, 32], [117, 88, 32]], [[54, 238, 11]], [[242, 176, 221]], [[260, 163, 40]], [[94, 215, 232], [132, 104, 215]], [[57, 9, 174]], [[78, 74, 221]], [[127, 282, 32]], [[47, 229, 52], [13, 115, 225]], [[11, 282, 32]]]\n",
      "[[-1], [150], [48, 251], [32], [161], [81], [62, 38], [-1], [128], [155, 258], [136, 76], [-1], [-1], [53], [-1], [12], [11, 280], [-1], [182, 252], [-1], [272, 117], [-1], [242], [260], [94, 132], [223], [-1], [127], [47, 13], [-1]]\n",
      "[[[19.37, 19.92, 20.29]], [], [[21.66, 22.85, 22.87], [16.22, 21.29, 21.83]], [[17.7, 19.69, 20.2]], [[14.42, 17.94, 18.41]], [[14.81, 19.98, 20.56]], [[24.94, 25.21, 25.45], [24.77, 25.75, 26.04]], [[19.03, 19.18, 19.71]], [[20.6, 20.74, 20.82]], [[19.06, 21.88, 21.92], [27.03, 27.97, 27.98]], [[23.7, 25.62, 25.75], [25.57, 25.72, 25.77]], [[17.12, 17.14, 17.31]], [[19.42, 19.49, 19.63]], [[16.54, 18.25, 18.84]], [[18.5, 19.57, 19.63]], [[19.72, 20.82, 20.95]], [[19.35, 19.7, 19.98], [26.79, 26.88, 27.1]], [], [[26.4, 26.59, 26.85], [24.2, 24.5, 24.86]], [[16.64, 17.5, 17.61]], [[22.04, 25.47, 25.77], [19.88, 25.29, 25.38]], [[18.06, 18.12, 18.37]], [[18.45, 24.3, 24.49]], [[13.44, 21.44, 21.87]], [[19.52, 22.62, 22.85], [18.08, 21.4, 21.58]], [[22.03, 22.35, 22.37]], [[17.42, 17.98, 18.2]], [[18.9, 19.62, 20.39]], [[24.14, 24.37, 24.69], [22.75, 24.15, 24.28]], [[18.14, 19.1, 19.32]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "\n",
    "res = []\n",
    "distances_res = []\n",
    "for i in tqdm(range(30)):\n",
    "    picture_res = []\n",
    "    picture_distances = []\n",
    "    \n",
    "    # For each painting inside an image from hog_query[i]\n",
    "    for j in range(len(hog_query[i])):\n",
    "        distances = []\n",
    "\n",
    "        query_descriptor = hog_query[i][j]['descriptor']\n",
    "        for p in range(len(hog_bd)): \n",
    "            bd_descriptor = hog_bd[p]['descriptor']\n",
    "            distance = euclidean(query_descriptor, bd_descriptor)\n",
    "            distances.append((distance, p))\n",
    "\n",
    "        distances.sort(key=lambda x: x[0])\n",
    "\n",
    "        nearest_neighbors = [idx for _, idx in distances[:k]] \n",
    "        nearest_distances = [round(dist, 2) for dist, _ in distances[:k]]\n",
    "\n",
    "        picture_res.append(nearest_neighbors)\n",
    "        picture_distances.append(nearest_distances)\n",
    "\n",
    "    res.append(picture_res)\n",
    "    distances_res.append(picture_distances)\n",
    "    \n",
    "print(res)\n",
    "print(ground_truth)\n",
    "print(distances_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 [40, 282, 11]\n",
      "48 [48, 254, 238]\n",
      "251 [251, 282, 11]\n",
      "32 [32, 11, 282]\n",
      "161 [161, 11, 40]\n",
      "81 [81, 40, 11]\n",
      "62 [54, 238, 202]\n",
      "38 [38, 182, 229]\n",
      "-1 [282, 40, 32]\n",
      "128 [128, 54, 238]\n",
      "155 [155, 11, 282]\n",
      "258 [258, 215, 57]\n",
      "136 [136, 137, 54]\n",
      "76 [21, 92, 54]\n",
      "-1 [78, 21, 74]\n",
      "-1 [32, 11, 282]\n",
      "53 [53, 238, 54]\n",
      "-1 [93, 197, 198]\n",
      "12 [12, 21, 74]\n",
      "11 [282, 11, 245]\n",
      "280 [232, 282, 32]\n",
      "182 [233, 282, 51]\n",
      "252 [215, 57, 259]\n",
      "-1 [93, 198, 78]\n",
      "272 [272, 282, 32]\n",
      "117 [117, 88, 32]\n",
      "-1 [54, 238, 11]\n",
      "242 [242, 176, 221]\n",
      "260 [260, 163, 40]\n",
      "94 [94, 215, 232]\n",
      "132 [132, 104, 215]\n",
      "223 [57, 9, 174]\n",
      "-1 [78, 74, 221]\n",
      "127 [127, 282, 32]\n",
      "47 [47, 229, 52]\n",
      "13 [13, 115, 225]\n",
      "-1 [11, 282, 32]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5675675675675675"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import src.metrics \n",
    "importlib.reload(src.metrics)\n",
    "\n",
    "from src.metrics import mapk\n",
    "\n",
    "mapk(ground_truth, res, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DAISY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import daisy\n",
    "\n",
    "def daisy_descriptor(image, step=250, radius=50, rings=3, histograms=6, orientations=8):\n",
    "    # Convertimos la imagen a escala de grises\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculamos los descriptores DAISY sin visualización\n",
    "    descriptors, dsc_image = daisy(\n",
    "        gray_image,\n",
    "        step=step,\n",
    "        radius=radius,\n",
    "        rings=rings,\n",
    "        histograms=histograms,\n",
    "        orientations=orientations,\n",
    "        visualize=True\n",
    "    )\n",
    "    \n",
    "    return descriptors, dsc_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daisy_descriptor(image, step=158, radius=50, rings=3, histograms=6, orientations=8):\n",
    "    # Convertimos la imagen a escala de grises\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Usamos un detector de keypoints (aquí FAST, pero puedes usar otros)\n",
    "    fast = cv2.FastFeatureDetector_create()\n",
    "    keypoints = fast.detect(gray_image, None)\n",
    "    \n",
    "    # Calculamos los descriptores DAISY en los puntos clave detectados\n",
    "    descriptors = daisy(\n",
    "        gray_image,\n",
    "        step=step,\n",
    "        radius=radius,\n",
    "        rings=rings,\n",
    "        histograms=histograms,\n",
    "        orientations=orientations,\n",
    "        visualize=False\n",
    "    )\n",
    "    \n",
    "    # Alineamos los descriptores con los keypoints (si es necesario)\n",
    "    keypoints_daisy = []\n",
    "    keypoint_descriptors = []\n",
    "    \n",
    "    for kp in keypoints:\n",
    "        # Para cada keypoint, calculamos su posición (en coordenadas de píxeles)\n",
    "        y, x = kp.pt\n",
    "        x, y = int(x), int(y)\n",
    "        \n",
    "        # Aquí usamos una verificación simple, que si el keypoint está dentro de la imagen\n",
    "        if x < descriptors.shape[1] and y < descriptors.shape[0]:\n",
    "            keypoints_daisy.append(kp)\n",
    "            keypoint_descriptors.append(descriptors[y, x, :])  # Tomamos el descriptor en esa ubicación\n",
    "    \n",
    "    # Convertimos las listas a arrays de numpy para facilitar el uso\n",
    "    keypoints_daisy = np.array(keypoints_daisy)\n",
    "    keypoint_descriptors = np.array(keypoint_descriptors)\n",
    "    \n",
    "    return keypoints_daisy, keypoint_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daisy_query = []\n",
    "for picture in tqdm(QSD1_w4_filtered[:4]):\n",
    "    for painting in picture:\n",
    "        kp, des = daisy_descriptor(painting)\n",
    "        daisy_query.append({'kp': kp, 'des': des})\n",
    "\n",
    "\n",
    "daisy_bd = []\n",
    "for picture in tqdm(QSD1_w4_nonAugmented[:4]):\n",
    "    for painting in picture:\n",
    "        kp, des = daisy_descriptor(painting)\n",
    "        daisy_bd.append({'kp': kp, 'des': des})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la imagen original y la visualización del descriptor DAISY\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('Imagen original')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(dsc_image, cmap='gray')\n",
    "plt.title('Descriptor DAISY')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 2\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image1, cmap='gray')\n",
    "plt.title('Imagen original')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(dsc_image1, cmap='gray')\n",
    "plt.title('Descriptor DAISY')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir descriptores a tipo float32 para el matcher de OpenCV\n",
    "descriptors = des_daisy.astype(np.float32).reshape(-1, 152)\n",
    "descriptors1 = des_daisy1.astype(np.float32).reshape(-1, 152)\n",
    "\n",
    "# Crear el matcher Brute-Force con la métrica de distancia Euclidiana\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "# Encontrar las mejores coincidencias\n",
    "matches = bf.match(descriptors, descriptors1)\n",
    "\n",
    "# Ordenar las coincidencias por distancia (las más cercanas son las mejores)\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "# Para dibujar los keypoints necesitamos crear un arreglo de KeyPoint\n",
    "# Generar keypoints para la primera y segunda imagen a partir de la malla DAISY\n",
    "keypoints = []\n",
    "keypoints1 = []\n",
    "\n",
    "step = 158  # El valor del 'step' usado en DAISY\n",
    "radius = 50  # El radio del descriptor\n",
    "\n",
    "# Generar keypoints para la imagen original\n",
    "for i in range(des_daisy.shape[0]):\n",
    "    for j in range(des_daisy.shape[1]):\n",
    "        keypoints.append(cv2.KeyPoint(j * step + radius, i * step + radius, 1))\n",
    "\n",
    "# Generar keypoints para la segunda imagen\n",
    "for i in range(des_daisy1.shape[0]):\n",
    "    for j in range(des_daisy1.shape[1]):\n",
    "        keypoints1.append(cv2.KeyPoint(j * step + radius, i * step + radius, 1))\n",
    "\n",
    "# Dibujar los matches entre las imágenes\n",
    "img_matches = cv2.drawMatches(\n",
    "    image, keypoints,  # Imagen original y sus keypoints\n",
    "    image1, keypoints1,  # Imagen de referencia y sus keypoints\n",
    "    matches[:30], None\n",
    ")\n",
    "\n",
    "# Mostrar el resultado con matplotlib\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img_matches)\n",
    "plt.title('Coincidencias entre imagen 1 y imagen 2 usando DAISY')\n",
    "plt.axis('off')  # Desactivar los ejes para que sea más limpio\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_query = []\n",
    "for picture in tqdm(QSD1_w4_filtered):\n",
    "    for painting in picture:\n",
    "        kp, des = surf_descriptor(painting)\n",
    "        surf_query.append({'kp': kp, 'des': des})\n",
    "\n",
    "\n",
    "surf_bd = []\n",
    "for picture in tqdm(QSD1_w4_nonAugmented):\n",
    "    for painting in picture:\n",
    "        kp, des = surf_descriptor(painting)\n",
    "        surf_bd.append({'kp': kp, 'des': des})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orb_descriptor(image, params={}):\n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    orb = cv2.ORB_create(**params)\n",
    "    kp, des = orb.detectAndCompute(img_gray, None)\n",
    "\n",
    "    return (kp, des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Find tentative matches based on similarity of local appearance and verify matches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Implement a system to discard queries not in the data set (unknowns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Evaluate system based on keypoint descriptors on QSD1-W4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Compare your best query system from previous week on QSD1-W4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Create pkl file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
